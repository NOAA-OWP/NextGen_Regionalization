{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import os.path\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import apply_donor_constraints\n",
    "from sklearn.cluster import KMeans, DBSCAN, Birch\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "import math\n",
    "import hdbscan\n",
    "\n",
    "import my_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read configuration (algorithm parameters etc) into dictionary\n",
    "with open('../data/config.yaml', 'r') as stream:\n",
    "    try:\n",
    "        config = yaml.safe_load(stream)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n",
    "\n",
    "# read donor attributes\n",
    "dtAttrDonor = pd.read_csv('../data/all_attrs_donors.csv')\n",
    "dtAttrDonor['tag'] = 'donor'\n",
    "\n",
    "# read receiver attributes\n",
    "dtAttrReceiver = pd.read_csv('../data/all_attrs_receivers.csv')\n",
    "dtAttrReceiver['tag'] = 'receiver'\n",
    "\n",
    "# if donors overlap with receivers (i.e., they share the same hydrofabric files), exclude them from the receivers\n",
    "if config['hydrofabric']['shared']:\n",
    "    dtAttrReceiver = dtAttrReceiver.loc[~dtAttrReceiver['id'].isin(dtAttrDonor['id'])]\n",
    "\n",
    "# combine donor & receiver attributes\n",
    "dtAttrAll = pd.concat([dtAttrDonor, dtAttrReceiver])\n",
    "\n",
    "# detemine whether the catchment is snowy (as snowy and non-snowy catchments are processed separately)\n",
    "dtAttrAll['snowy'] = dtAttrAll['snow_frac'].apply(lambda x: True if x >= config['pars']['general']['minSnowFrac'] else False)\n",
    "del dtAttrDonor, dtAttrReceiver\n",
    "\n",
    "# columns in the attrs table that are not actual attributes\n",
    "config['non_attr_cols'] = [\"id\",\"tag\",\"snowy\",\"hsg\"]\n",
    "\n",
    "# pretty print nested dictionaries\n",
    "# print(yaml.dump(config, default_flow_style=False)) \n",
    "\n",
    "# compute spatial distance between all receivers and donors if not already exists\n",
    "f1 = '../data/dist_spatial_donor_receiver.csv'\n",
    "if os.path.isfile(f1):\n",
    "    dist_spatial = pd.read_csv(f1,index_col=0)\n",
    "else:\n",
    "    pass\n",
    "    #distSpatial0 = compute_dist_spatial(f1, subset(dtAttrAll,tag==\"donor\")$id, subset(dtAttrAll,tag==\"receiver\")$id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excluding 4 attributes: sand_frac,clay_frac,soil_porosity,soil_conductivity\n",
      "Number of PCs selected: 6\n",
      "PCA total portion of variance explained ... 0.839470613290475\n",
      "weights: [0.39203068 0.27393743 0.12728765 0.08450586 0.06471906 0.0575193 ]\n",
      "168 receivers to be processed this round\n"
     ]
    }
   ],
   "source": [
    "scenario = 'camels'\n",
    "dtDonorAll = pd.DataFrame()    \n",
    "\n",
    "# attributes to be used\n",
    "attrs1 = config['attrs'][scenario]\n",
    "\n",
    "# all receivers to find donors for\n",
    "recs0 = dtAttrAll[dtAttrAll['tag']=='receiver']['id'].tolist()\n",
    "recs1 = list() #recerivers that have already been processed in previous rounds\n",
    "\n",
    "# reduce the attribute table to attributes\n",
    "dtAttr0 = dtAttrAll[config['non_attr_cols']+attrs1]\n",
    "\n",
    "# figure out valid attributes to use this round\n",
    "dtAttr = my_utils.get_valid_attrs(recs0, recs1, dtAttr0, attrs1, config)\n",
    "\n",
    "# apply principal component analysis\n",
    "myscores, weights = my_utils.apply_pca(dtAttr.drop(config['non_attr_cols'], axis=1), config['pars']['general']['minVarPCA'])    \n",
    "print(\"weights: \" + str(weights))\n",
    "        \n",
    "# donors and receivers for this round\n",
    "receiversAll1 = dtAttr[dtAttr['tag']=='receiver']['id'].tolist()\n",
    "\n",
    "# determine which receivers to be processed for the current round\n",
    "# process only those not-yet processed receivers\n",
    "recs = [r1 for r1 in recs0 if r1 in receiversAll1 and r1 not in recs1]\n",
    "recs1 = recs1 + recs\n",
    "print(str(len(recs)) + \" receivers to be processed this round\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process snowy and non-snowy catchments sparately\n",
    "snow1 = False\n",
    "method = 'hdbscan'\n",
    "    \n",
    "# the current snowy group\n",
    "dtAttr1 = dtAttr[dtAttr['snowy']==snow1]\n",
    "donors = dtAttr1[dtAttr1['tag']=='donor']['id'].tolist()\n",
    "receivers = dtAttr1[dtAttr1['tag']=='receiver']['id'].tolist()\n",
    "receivers = [x for x in receivers if x in recs]\n",
    "\n",
    "# scores for donors and and receivers for this round\n",
    "# keep1 = (dtAttr1.tag==\"donor\") | ((dtAttr1.tag==\"receiver\") & (dtAttr1.id.isin(receivers)))  \n",
    "# dtAttr1 = dtAttr1[keep1.values]     \n",
    "# scores1 = myscores[(dtAttr['snowy']==snow1).values][keep1.values]\n",
    "\n",
    "scores1 = myscores[(dtAttr['snowy']==snow1).values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify donors iteratively\n",
    "dfDonors = pd.DataFrame()\n",
    "kround = 0\n",
    "donors1 = donors.copy()\n",
    "nrec_with_donor0 = dfDonors.shape[0]\n",
    "labels = np.zeros(scores1.shape[0])\n",
    "label0 = -99 #labels indicating donor identified\n",
    "\n",
    "while dfDonors.shape[0] < len(receivers):\n",
    "    \n",
    "    kround = kround + 1\n",
    "    print('\\n=========== Round=' + str(kround) + ' =====================')\n",
    "    \n",
    "    label_rec, count_rec = np.unique(labels[len(donors):], return_counts=True) # receiver clusters\n",
    "    label_don, count_don = np.unique(labels[:len(donors)], return_counts=True) # donor clusters\n",
    "    count_rec = count_rec[label_rec != label0]; label_rec = label_rec[label_rec != label0]\n",
    "    \n",
    "    print('label_rec: ' + str(label_rec))\n",
    "    \n",
    "    labels1 = labels.copy()\n",
    "    for ll in label_rec:\n",
    "        donors1 = [x for jj,x in enumerate(donors) if labels1[jj]==ll]\n",
    "        receivers1 =[ x for jj,x in enumerate(receivers) if labels1[jj+len(donors)]==ll]\n",
    "        \n",
    "        print('Number of donors in cluster ' + str(ll) + ': ' + str(len(donors1)))\n",
    "        \n",
    "        if len(donors1)>0:\n",
    "            if len(donors1) > config['pars'][method]['nDonorMax']:\n",
    "                idx1 = [donors.index(x) for x in donors1]\n",
    "                idx2 = [receivers.index(x) for x in receivers1]\n",
    "                idx0 = idx1 + [x+len(donors) for x in idx2]\n",
    "                mydata1 = scores1.iloc[idx0]\n",
    "                \n",
    "                fit1 = hdbscan.HDBSCAN(min_samples=11,min_cluster_size=config['pars'][method]['minClusterSize'],allow_single_cluster=False).fit(mydata1)\n",
    "                fit1.labels_ = fit1.labels_ + 2\n",
    "                label_rec1, count_rec1 = np.unique(fit1.labels_[len(donors1):], return_counts=True) # receiver clusters\n",
    "                label_don1, count_don1 = np.unique(fit1.labels_[:len(donors1)], return_counts=True) # donor clusters\n",
    "                labels[idx0] = fit1.labels_ + labels1.max()\n",
    "                print(\"donor clusters: \" + str(label_don1))\n",
    "                print(\"donor cluster counts: \" + str(count_don1))\n",
    "                print(\"receiver clusters: \" + str(label_rec1))\n",
    "                print(\"receiver cluster counts: \" + str(count_rec1))\n",
    "            else:\n",
    "                dfDonors = pd.concat((dfDonors, my_utils.assign_donors(scenario, donors1, receivers1, config['pars']['general'], dist_spatial, dtAttrAll)),axis=0)\n",
    "                labels[labels == ll] = label0  \n",
    "        else: # note proximity chooses from all donors\n",
    "            dfDonors = pd.concat((dfDonors, my_utils.assign_donors('proximity', donors, receivers1, config['pars']['general'], dist_spatial, dtAttrAll)),axis=0)            \n",
    "            labels[labels == ll] = label0           \n",
    "       \n",
    "    # Algorithm converged or no donors available for next round\n",
    "    if (nrec_with_donor0 == dfDonors.shape[0]) & (kround>3):        \n",
    "        recs2 = [x for x in receivers if x not in dfDonors['id']]\n",
    "        if len(recs2) > 0:\n",
    "            print(\"\\nAlgorithm converged with donors unidentified for some receivers ... use proximity for these receivers\")\n",
    "            dfDonors = pd.concat((dfDonors, my_utils.assign_donors('proximity', donors, recs2, config['pars']['general'], dist_spatial, dtAttrAll)),axis=0)    \n",
    "            \n",
    "    # update progress\n",
    "    print(\"Number of receivers with donors identified: \" + str(dfDonors.shape[0]))\n",
    "    if dfDonors.shape[0] > 0:\n",
    "        uniq, freq = np.unique(dfDonors['tag'],return_counts=True)\n",
    "        print(dict(zip(uniq, freq)))\n",
    "\n",
    "    # update number of receivers with donors identified\n",
    "    nrec_with_donor0 = dfDonors.shape[0]\n",
    "    \n",
    "\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "68\n",
      "64\n"
     ]
    }
   ],
   "source": [
    "recs2 = [x for x in receivers if x not in dfDonors['id'].tolist()]\n",
    "print(len(recs2))\n",
    "print(len(receivers))\n",
    "print(len(dfDonors['id']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
